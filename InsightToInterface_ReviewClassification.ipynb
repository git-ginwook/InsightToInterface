{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMAk9Tt7lOgCf2SY0Dd1TuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-ginwook/InsightToInterface/blob/movies_bert/InsightToInterface_ReviewClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Classification"
      ],
      "metadata": {
        "id": "DGp-FxOA6c5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "14WdVg4E66sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/code)\n",
        "\n",
        "- connect dataset directly from Kaggle"
      ],
      "metadata": {
        "id": "NEvsKUZn3AAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "zKF_6g3f5H45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "reviews_df = pd.read_csv(path + \"/IMDB Dataset.csv\")\n",
        "reviews_df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vN4Y1U735WGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Device Setup"
      ],
      "metadata": {
        "id": "idAjF2Ep98yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Verify A100 GPU set up"
      ],
      "metadata": {
        "id": "kd9uwWaH-tzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Automatically selects the first available GPU\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "\n",
        "    # Check if it's an A100 GPU\n",
        "    if \"A100\" in gpu_name:\n",
        "        print(f\"Successfully set up {gpu_name}!\")\n",
        "    else:\n",
        "        print(f\"GPU assigned: {gpu_name}. Note: It's not an A100 GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available. Using CPU.\")\n",
        "\n",
        "# Print CUDA version\n",
        "print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n"
      ],
      "metadata": {
        "id": "a6chhxqX-DUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Verify GPU performance"
      ],
      "metadata": {
        "id": "jWLuXQv9-8Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Dummy tensor operation to benchmark GPU\n",
        "device = torch.device(\"cuda\")\n",
        "size = 10000\n",
        "\n",
        "# Create random tensors\n",
        "a = torch.randn(size, size, device=device)\n",
        "b = torch.randn(size, size, device=device)\n",
        "\n",
        "# Time matrix multiplication on GPU\n",
        "start = time.time()\n",
        "c = torch.matmul(a, b)\n",
        "torch.cuda.synchronize()  # Wait for GPU to finish\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Time for matrix multiplication on GPU: {end - start:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "IVRpDrpt--OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "FYTr7QrE62wM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[Customer Segmentation using LLMs: Advanced Clustering Techniques for Effective Targeting](https://ai.plainenglish.io/customer-segmentation-using-llms-advanced-clustering-techniques-for-effective-targeting-493116116ab6)"
      ],
      "metadata": {
        "id": "OPIjiegz3QT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. BERT for Text Embedding -> review sentiments (e.g., + or -)"
      ],
      "metadata": {
        "id": "DEclc5Iz6lqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device) # use GPU\n",
        "\n",
        "# Sample customer reviews\n",
        "customer_reviews = reviews_df[\"review\"].to_list()\n",
        "\n",
        "# Tokenize and encode the reviews\n",
        "# inputs = tokenizer(customer_reviews, return_tensors='pt', padding=True, truncation=True)\n",
        "inputs = tokenizer(customer_reviews, return_tensors='pt', padding=True, truncation=True)\n",
        "inputs.to(device) # load data to GPU\n",
        "\n",
        "# Get the embeddings from the BERT model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Convert embeddings to a numpy array for clustering\n",
        "embeddings = embeddings.cpu().numpy() # bring back output from GPU to CPU\n",
        "\n",
        "# Perform K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Add the results to a DataFrame for better visualization\n",
        "df = pd.DataFrame({\n",
        "    'Review': customer_reviews,\n",
        "    'Cluster': labels\n",
        "})\n",
        "\n",
        "# Show the clusters\n",
        "print(df)"
      ],
      "metadata": {
        "id": "2iqa9VtO3EKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Latent Dirichlet Allocation (LDA) -> review themes (e.g., price, quality)\n",
        "- Not working yet. need to investigate"
      ],
      "metadata": {
        "id": "G8Z4q_ii5T5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Sample Dataset\n",
        "documents = [\n",
        "    \"Machine learning is fascinating.\",\n",
        "    \"Artificial intelligence and deep learning are branches of machine learning.\",\n",
        "    \"I like icecream.\",\n",
        "    \"AI and machine learning are transforming industries.\",\n",
        "    \"Blueberry is delicious.\"\n",
        "]\n",
        "\n",
        "# Step 1: Text Preprocessing (e.g., Tokenization, Stopword Removal, Vectorization)\n",
        "vectorizer = CountVectorizer(\n",
        "    max_df=0.95,  # Ignore terms with a document frequency > 95%\n",
        "    min_df=2,     # Ignore terms with a document frequency < 2\n",
        "    stop_words='english'  # Remove common stopwords\n",
        ")\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 2: Apply LDA\n",
        "n_topics = 2  # Specify the number of topics\n",
        "lda_model = LatentDirichletAllocation(\n",
        "    n_components=n_topics,   # Number of topics\n",
        "    max_iter=10,             # Maximum number of iterations\n",
        "    learning_method='batch', # Batch or online learning\n",
        "    random_state=42          # Random seed for reproducibility\n",
        ")\n",
        "lda_model.fit(X)\n",
        "\n",
        "# Step 3: Display Topics\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {topic_idx + 1}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 5  # Number of top words to display per topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "display_topics(lda_model, feature_names, no_top_words)\n",
        "\n",
        "# Step 4: Topic Distribution for Documents\n",
        "doc_topic_distribution = lda_model.transform(X)\n",
        "print(\"\\nDocument-Topic Distributions:\")\n",
        "print(doc_topic_distribution)\n"
      ],
      "metadata": {
        "id": "kXQ3y7nk7cOW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}