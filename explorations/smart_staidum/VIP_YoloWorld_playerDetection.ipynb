{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9CTEakjcc7oWQM213rjfc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-ginwook/InsightToInterface/blob/main/explorations/smart_staidum/VIP_YoloWorld_playerDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultalytics Documentation: https://docs.ultralytics.com/quickstart/#install-ultralytics\n",
        "https://docs.ultralytics.com/models/yolo-world/#train-usage"
      ],
      "metadata": {
        "id": "eVaqZjCgZKHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "XDoM--4MaG1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uc4LuRJZGF9"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLOWorld\n",
        "\n",
        "# # Load a pretrained YOLOv8s-worldv2 model\n",
        "# model = YOLOWorld(\"/content/yolov8s-worldv2.pt\")\n",
        "\n",
        "# Load a pretrained YOLOv8x-worldv2 model\n",
        "model = YOLOWorld(\"/yolov8x-worldv2.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecting objects in image"
      ],
      "metadata": {
        "id": "ycxrLMw9e4Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference with the YOLOv8n model on the 'bus.jpg' image\n",
        "# img_results = model(\"test.jpg\")\n",
        "img_results = model(\"2017_G6017_Q1_01.jpeg\")"
      ],
      "metadata": {
        "id": "NZBsCVHae3s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_results[0].show()\n"
      ],
      "metadata": {
        "id": "sYTMmBMscXt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tracking objects on video"
      ],
      "metadata": {
        "id": "-KWzhNRmeo25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Track with a YOLO-World model on a video\n",
        "video_results = model.track(source=\"005.mp4\")"
      ],
      "metadata": {
        "id": "wJyPju5CenNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show zero-shot for the starting frame (0th frame)\n",
        "video_results[0].show()"
      ],
      "metadata": {
        "id": "Byw6Cl0qgHQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Frame\n",
        "- filter for 'person' only"
      ],
      "metadata": {
        "id": "Hi11LcpR71FM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract results from the first frame\n",
        "first_frame_result = video_results[0]\n",
        "\n",
        "# Check if there are detections in the first frame\n",
        "if first_frame_result is not None and first_frame_result.boxes is not None:\n",
        "    # Extract bounding boxes, class IDs, and confidence scores\n",
        "    boxes = first_frame_result.boxes.xyxy.tolist()\n",
        "    class_ids = first_frame_result.boxes.cls.tolist()\n",
        "    confidences = first_frame_result.boxes.conf.tolist()\n",
        "\n",
        "    # Read original frame\n",
        "    frame = first_frame_result.orig_img  # Original image from results\n",
        "\n",
        "    # Iterate over detections and draw only 'person' bounding boxes\n",
        "    for i, class_id in enumerate(class_ids):\n",
        "        label = model.names[int(class_id)]  # Convert class ID to label\n",
        "        if label == 'person':  # Filter only 'person' detections\n",
        "            x1, y1, x2, y2 = map(int, boxes[i])  # Get bbox coordinates\n",
        "            conf = confidences[i]  # Confidence score\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            text = f\"{label} {conf:.2f}\"\n",
        "            cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Convert BGR (OpenCV format) to RGB (Matplotlib format)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the frame\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(frame_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"First Frame - Person Detections\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No detections found in the first frame.\")\n"
      ],
      "metadata": {
        "id": "sUZ-BKY-7mTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALL frames\n",
        "- filter for 'person' only"
      ],
      "metadata": {
        "id": "_SrGhy2i7zM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to filter detections for the 'person' class\n",
        "def filter_person_detections(video_results):\n",
        "    person_detections = []\n",
        "\n",
        "    for result in video_results:  # Iterate through results (one per frame)\n",
        "        if result is not None and result.boxes is not None:\n",
        "            boxes = result.boxes  # Get bounding boxes\n",
        "            class_ids = boxes.cls.tolist()  # Convert class tensor to list\n",
        "            confidences = boxes.conf.tolist()  # Convert confidence tensor to list\n",
        "            bboxes = boxes.xyxy.tolist()  # Convert bounding boxes tensor to list\n",
        "\n",
        "            # Iterate over detected objects\n",
        "            for i, class_id in enumerate(class_ids):\n",
        "                label = model.names[int(class_id)]  # Convert class ID to label\n",
        "                if label == 'person':  # Filter only 'person' detections\n",
        "                    person_detections.append({\n",
        "                        \"bbox\": bboxes[i],  # Bounding box coordinates (x1, y1, x2, y2)\n",
        "                        \"confidence\": confidences[i],  # Confidence score\n",
        "                        \"class\": label\n",
        "                    })\n",
        "\n",
        "    return person_detections\n",
        "\n",
        "# Get filtered 'person' detections\n",
        "person_detections = filter_person_detections(video_results)\n",
        "\n",
        "# Print or save the results\n",
        "for detection in person_detections:\n",
        "    print(detection)"
      ],
      "metadata": {
        "id": "g0mlWGYg7Is3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}