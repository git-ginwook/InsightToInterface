{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true,"authorship_tag":"ABX9TyM6HWygYLl6uRm2ByZR3R0r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SAM2 Video Predictor\n","https://github.com/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb"],"metadata":{"id":"qTt94oA5lUi4"}},{"cell_type":"markdown","source":["## Environment Setup"],"metadata":{"id":"Ak5kcMoQbleB"}},{"cell_type":"markdown","source":["### Import from Hugging Face"],"metadata":{"id":"GOn3WGHyhN0_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwdUg6lSWAL7"},"outputs":[],"source":["try:\n","    import huggingface_hub\n","    print(\"`huggingface_hub` is installed.\")\n","except ImportError:\n","    print(\"`huggingface_hub` is not installed.\")\n"]},{"cell_type":"code","source":["using_colab = True"],"metadata":{"id":"vtLeL3M_bjWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if using_colab:\n","    import torch\n","    import torchvision\n","    print(\"PyTorch version:\", torch.__version__)\n","    print(\"Torchvision version:\", torchvision.__version__)\n","    print(\"CUDA is available:\", torch.cuda.is_available())\n","    import sys\n","    !{sys.executable} -m pip install opencv-python matplotlib\n","    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'"],"metadata":{"collapsed":true,"id":"ZSymbRVCboV5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GPU Setup\n","- Connect A100 or L4 GPUs"],"metadata":{"id":"8kqc6kRGcONj"}},{"cell_type":"code","source":["import os\n","# if using Apple MPS, fall back to CPU for unsupported ops\n","os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","from PIL import Image"],"metadata":{"id":"WWLg8b3UklEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select the device for computation\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","print(f\"using device: {device}\")\n","\n","if device.type == \"cuda\":\n","    # use bfloat16 for the entire notebook\n","    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n","    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n","    if torch.cuda.get_device_properties(0).major >= 8:\n","        torch.backends.cuda.matmul.allow_tf32 = True\n","        torch.backends.cudnn.allow_tf32 = True\n","elif device.type == \"mps\":\n","    print(\n","        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n","        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n","        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n","    )"],"metadata":{"id":"5cUZK3A-cNWM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SAM2 Checkpoint Model"],"metadata":{"id":"gHNyZdhng1aA"}},{"cell_type":"code","source":["    # !mkdir -p videos\n","    # !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n","    # !unzip -d videos videos/bedroom.zip\n","\n","    !mkdir -p ../checkpoints/\n","    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"],"metadata":{"id":"I6v559-Ggtaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading the SAM 2 video predictor"],"metadata":{"id":"UhdAwL_egEkM"}},{"cell_type":"code","source":["from sam2.build_sam import build_sam2_video_predictor\n","\n","sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n","model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n","\n","predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"],"metadata":{"id":"HGw1sjmPbAOr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_mask(mask, ax, obj_id=None, random_color=False):\n","    if random_color:\n","        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n","    else:\n","        cmap = plt.get_cmap(\"tab10\")\n","        cmap_idx = 0 if obj_id is None else obj_id\n","        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n","    h, w = mask.shape[-2:]\n","    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n","    ax.imshow(mask_image)\n","\n","\n","def show_points(coords, labels, ax, marker_size=200):\n","    pos_points = coords[labels==1]\n","    neg_points = coords[labels==0]\n","    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n","    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n","\n","\n","def show_box(box, ax):\n","    x0, y0 = box[0], box[1]\n","    w, h = box[2] - box[0], box[3] - box[1]\n","    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"],"metadata":{"id":"UwF2slgOh0Wc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sample Video"],"metadata":{"id":"k59K57yXj2nf"}},{"cell_type":"markdown","source":["### Dataset\n","- temporary import (uploaded files get deleted once the runtime is recycled)"],"metadata":{"id":"hiMt5D66e9rz"}},{"cell_type":"code","source":["from google.colab import files\n","\n","# Upload a video file from your local machine\n","uploaded = files.upload()\n","\n","# List the uploaded files\n","for file_name in uploaded.keys():\n","    print(f\"Uploaded file: {file_name}\")"],"metadata":{"id":"8ES8QYbje9Wv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Convert video to JPEG frames (for a single video for now)"],"metadata":{"id":"VkpjsqLCiz4X"}},{"cell_type":"code","source":["# create output folder\n","clip = '2017_G6016_Q4_011'\n","os.makedirs(f\"/content/{clip}\", exist_ok=True)"],"metadata":{"id":"VrnFl4FbnA7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ffmpeg version check\n","!ffmpeg -version"],"metadata":{"id":"uJcTHYE6i4P7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ffmpeg -i /content/{clip}.mp4 -q:v 2 -start_number 0 /content/{clip}/%05d.jpg"],"metadata":{"id":"J8RCDFovi9Hu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Set frame directory"],"metadata":{"id":"Dg1RguGqkQdF"}},{"cell_type":"code","source":["# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n","video_dir = f\"/content/{clip}/\"\n","\n","# scan all the JPEG frame names in this directory\n","frame_names = [\n","    p for p in os.listdir(video_dir)\n","    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n","]\n","\n","frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))"],"metadata":{"id":"yo2memIckPw0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Display the first frame"],"metadata":{"id":"nI5Md8hekUQW"}},{"cell_type":"code","source":["# take a look the first video frame\n","frame_idx = 0\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"],"metadata":{"id":"OhrCZCvAkWKs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Initialize Inference State\n","- SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an inference state on this video."],"metadata":{"id":"dhujMWZpk29H"}},{"cell_type":"code","source":["inference_state = predictor.init_state(video_path=video_dir)"],"metadata":{"id":"swPYZhefk5fY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Segment and Track a Single Object"],"metadata":{"id":"pkx2eiiqlH11"}},{"cell_type":"code","source":["# reset if any previous session\n","predictor.reset_state(inference_state)"],"metadata":{"id":"bTMLjjLmlHJJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Identify xy-coordinates\n","- identify coordinates externally"],"metadata":{"id":"rok9wa9beqS1"}},{"cell_type":"markdown","source":["### Add positive clicks on a frame"],"metadata":{"id":"RAOqUdPpWjtg"}},{"cell_type":"markdown","source":["enable multi-prompts (i.e., multi-objects)"],"metadata":{"id":"_1Z2YxyRyV9Y"}},{"cell_type":"code","source":["prompts = {} # hold all the clicks we add for visualization"],"metadata":{"id":"HN7rJIzhxMBp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Player 1"],"metadata":{"id":"CVkmzLzSwCxj"}},{"cell_type":"code","source":["frame_idx = 0  # the frame index we interact with\n","obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n","\n","# Let's add a positive click to get started\n","points = np.array([[602, 320]], dtype=np.float32)\n","\n","# for labels, `1` means positive click and `0` means negative click\n","labels = np.array([1], np.int32)\n","prompts[obj_id] = points, labels\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","    inference_state=inference_state,\n","    frame_idx=frame_idx,\n","    obj_id=obj_id,\n","    points=points,\n","    labels=labels,\n",")\n","\n","# show the results on the current (interacted) frame\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n","show_points(points, labels, plt.gca())\n","for i, out_obj_id in enumerate(out_obj_ids):\n","    show_points(*prompts[out_obj_id], plt.gca())\n","    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"i3CYtClbWZ3X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Player 2"],"metadata":{"id":"ujBFl6FMwGBI"}},{"cell_type":"code","source":["frame_idx = 0  # the frame index we interact with\n","obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n","\n","# Let's add a positive click to get started\n","points = np.array([[488, 328]], dtype=np.float32)\n","\n","# for labels, `1` means positive click and `0` means negative click\n","labels = np.array([1], np.int32)\n","prompts[obj_id] = points, labels\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","    inference_state=inference_state,\n","    frame_idx=frame_idx,\n","    obj_id=obj_id,\n","    points=points,\n","    labels=labels,\n",")\n","\n","# show the results on the current (interacted) frame\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n","show_points(points, labels, plt.gca())\n","for i, out_obj_id in enumerate(out_obj_ids):\n","    show_points(*prompts[out_obj_id], plt.gca())\n","    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"I_9JS5nowGu8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add second positive click for the same object (if needed)"],"metadata":{"id":"Y_qchDNEsmoY"}},{"cell_type":"code","source":["# ann_frame_idx = 0  # the frame index we interact with\n","# ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n","\n","# # Let's add a 2nd positive click to refine the mask\n","# # sending all clicks (and their labels) to `add_new_points_or_box`\n","# points = np.array([[602, 320], [488, 328]], dtype=np.float32)\n","# # for labels, `1` means positive click and `0` means negative click\n","# labels = np.array([1, 1], np.int32)\n","# _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","#     inference_state=inference_state,\n","#     frame_idx=ann_frame_idx,\n","#     obj_id=ann_obj_id,\n","#     points=points,\n","#     labels=labels,\n","# )\n","\n","# # show the results on the current (interacted) frame\n","# plt.figure(figsize=(9, 6))\n","# plt.title(f\"frame {ann_frame_idx}\")\n","# plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n","# show_points(points, labels, plt.gca())\n","# show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"],"metadata":{"id":"ylkwAUV7sl5Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Add negative clicks on a frame (if needed)"],"metadata":{"id":"3h2_JY62WwvX"}},{"cell_type":"markdown","source":["### Propagate the prompts to get masklet"],"metadata":{"id":"xu9fnZNHlICA"}},{"cell_type":"markdown","source":["Display each frame with its masklet"],"metadata":{"id":"LtGfZc9Jmqeb"}},{"cell_type":"code","source":["# run propagation throughout the video and collect the results in a dict\n","video_segments = {}  # video_segments contains the per-frame segmentation results\n","\n","for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n","    video_segments[out_frame_idx] = {\n","        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n","        for i, out_obj_id in enumerate(out_obj_ids)\n","    }\n","\n","# render the segmentation results every few frames\n","vis_frame_stride = 30\n","plt.close(\"all\")\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"frame {out_frame_idx}\")\n","    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"mO5Jpzd2lM6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert frames to GIF"],"metadata":{"id":"rNicfR25mwCb"}},{"cell_type":"code","source":["# Initialize a list to store frames for the GIF\n","frames_list = []\n","\n","# Render the segmentation results every few frames\n","vis_frame_stride = 10\n","plt.close(\"all\")\n","\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    # Create figure and axis\n","    fig, ax = plt.subplots(figsize=(6, 4))\n","    ax.set_title(f\"Frame {out_frame_idx}\")\n","\n","    # Load and display the frame\n","    frame_path = os.path.join(video_dir, frame_names[out_frame_idx])\n","    img = Image.open(frame_path)\n","    ax.imshow(img)\n","\n","    # Overlay segmentation masks\n","    if out_frame_idx in video_segments:\n","        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","            show_mask(out_mask, ax, obj_id=out_obj_id)\n","\n","    # Convert figure to image and append to frames_list\n","    fig.canvas.draw()\n","    frame_img = np.array(fig.canvas.renderer.buffer_rgba())  # Convert figure to image\n","    frames_list.append(Image.fromarray(frame_img))  # Append the image to the list\n","\n","    plt.close(fig)  # Close figure to avoid memory leaks\n","\n","print(f\"Collected {len(frames_list)} frames for GIF generation.\")\n"],"metadata":{"id":"yZ1_raOOoS_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_gif_path = \"/content/SAM2Tracking.gif\"\n","\n","if frames_list:\n","    frames_list[0].save(\n","        output_gif_path, save_all=True, append_images=frames_list[1:], duration=100, loop=0\n","    )\n","    print(f\"GIF saved successfully: {output_gif_path}\")\n"],"metadata":{"id":"Ei2z-WsNozhs"},"execution_count":null,"outputs":[]}]}